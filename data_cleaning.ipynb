{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71b60330-c0e0-4511-881c-89932fb0b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "import unicodedata\n",
    "from html.parser import HTMLParser\n",
    "import emoji\n",
    "import unidecode\n",
    "from spacy.lang.en import English\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79bd0044-a171-4dd3-9454-990418b3f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_complete_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29ca4551-eb8f-4bd2-8e0e-777d1614abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_characters(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def remove_unicode_symbols(text):\n",
    "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'So')\n",
    "    return text\n",
    "\n",
    "def replace_multi_occurrences(text, filler):\n",
    "    \"\"\"Replaces multiple occurrences of filler with n filler\"\"\"\n",
    "    # only run if we have multiple occurrences of filler\n",
    "    if text.count(filler) <= 1:\n",
    "        return text\n",
    "    # pad fillers with whitespace\n",
    "    text = text.replace(f'{filler}', f' {filler} ')\n",
    "    # remove introduced duplicate whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    # find indices of occurrences\n",
    "    indices = []\n",
    "    for m in re.finditer(r'{}'.format(filler), text):\n",
    "        index = m.start()\n",
    "        indices.append(index)\n",
    "    # collect merge list\n",
    "    merge_list = []\n",
    "    for i, index in enumerate(indices):\n",
    "        if i > 0 and index - old_index == len(filler) + 1:\n",
    "            # found two consecutive fillers\n",
    "            if len(merge_list) > 0 and merge_list[-1][1] == old_index:\n",
    "                # extend previous item\n",
    "                merge_list[-1][1] = index\n",
    "                merge_list[-1][2] += 1\n",
    "            else:\n",
    "                # create new item\n",
    "                merge_list.append([old_index, index, 2])\n",
    "        old_index = index\n",
    "    # merge occurrences\n",
    "    if len(merge_list) > 0:\n",
    "        new_text = ''\n",
    "        pos = 0\n",
    "        for (start, end, count) in merge_list:\n",
    "            new_text += text[pos:start]\n",
    "            new_text += f'{count} {filler}'\n",
    "            pos = end + len(filler)\n",
    "        new_text += text[pos:]\n",
    "        text = new_text\n",
    "    return text\n",
    "\n",
    "def segment_sentences(text, args):\n",
    "    \"\"\"Uses spacy to segment text into sentences. Sentences which only consist of a filler will be merged with previous or following sentences\"\"\"\n",
    "    doc = nlp(text)\n",
    "    regex_fillers = r'(^\\d {username}$)|^{username}$|(^\\d {url}$)|^{url}$'.format(username=args.username_filler, url=args.url_filler)\n",
    "    num_tokens = len(doc)\n",
    "    sentences = [s.string.strip() for s in doc.sents]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if re.match(regex_fillers, sentence):\n",
    "            if i == 0 and len(sentences) > 1:\n",
    "                # prepend to next sentence\n",
    "                sentences[i+1] = f'{sentence} {sentences[i+1]}'\n",
    "            elif i > 0:\n",
    "                # add sentence to previous\n",
    "                sentences[i-1] += f' {sentence}'\n",
    "            # remove current\n",
    "            del sentences[i]\n",
    "    return sentences, num_tokens\n",
    "\n",
    "def asciify_emojis(text):\n",
    "    \"\"\"\n",
    "    Converts emojis into text aliases. E.g. üëç becomes :thumbs_up:\n",
    "    For a full list of text aliases see: https://www.webfx.com/tools/emoji-cheat-sheet/\n",
    "    \"\"\"\n",
    "    text = emoji.demojize(text)\n",
    "    return text\n",
    "\n",
    "def standardize_text(text):\n",
    "    \"\"\"\n",
    "    1) Escape HTML\n",
    "    2) Replaces some non-standard punctuation with standard versions. \n",
    "    3) Replace \\r, \\n and \\t with white spaces\n",
    "    4) Removes all other control characters and the NULL byte\n",
    "    5) Removes duplicate white spaces\n",
    "    \"\"\"\n",
    "    # escape HTML symbols\n",
    "    #text = html_parser.unescape(text)\n",
    "    # standardize punctuation\n",
    "    text = text.translate(transl_table)\n",
    "    text = text.replace('‚Ä¶', '...')\n",
    "    # replace \\t, \\n and \\r characters by a whitespace\n",
    "    text = re.sub(control_char_regex, ' ', text)\n",
    "    # remove all remaining control characters\n",
    "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
    "    # replace multiple spaces with single space\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def standardize_punctuation(text):\n",
    "    return ''.join([unidecode.unidecode(t) if unicodedata.category(t)[0] == 'P' else t for t in text])\n",
    "\n",
    "def replace_usernames(text, filler='user'):\n",
    "    # @<user> is a marker used internally. use filler instead\n",
    "    text = text.replace('@<user>', f'{filler}')\n",
    "    # replace other user handles by filler\n",
    "    text = re.sub(username_regex, filler, text)\n",
    "    # add spaces between, and remove double spaces again\n",
    "    text = text.replace(filler, f' {filler} ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def replace_urls(text, filler='url'):\n",
    "    # <url> is a marker used internally. use filler instead\n",
    "    text = text.replace('<url>', filler)\n",
    "    # replace other urls by filler\n",
    "    text = re.sub(url_regex, filler, text)\n",
    "    # add spaces between, and remove double spaces again\n",
    "    text = text.replace(filler, f' {filler} ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10796fca-c0e9-4b40-8e23-ff2259894edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "976bdfaf-150a-406d-970f-93ddb9beebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"avoid_sentencizer_hashtags\")\n",
    "def _avoid_sentence_boundary_on_hashtag(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '#':\n",
    "            doc[token.i+1].is_sent_start = False\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5571a49-8312-413a-ab1f-df155ae3f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spacy_model():\n",
    "    nlp = English()\n",
    "    #sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "    #nlp.add_pipe(sentencizer)\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    nlp.add_pipe(\"avoid_sentencizer_hashtags\")\n",
    "    return nlp\n",
    "nlp = build_spacy_model()\n",
    "# compile regexes\n",
    "username_regex = re.compile(r'(^|[^@\\w])@(\\w{1,15})\\b')\n",
    "url_regex = re.compile(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))')\n",
    "control_char_regex = re.compile(r'[\\r\\n\\t]+')\n",
    "# translate table for punctuation\n",
    "transl_table = dict([(ord(x), ord(y)) for x, y in zip(u\"‚Äò‚Äô¬¥‚Äú‚Äù‚Äì-\",  u\"'''\\\"\\\"--\")])\n",
    "# HTML parser\n",
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "194fe93d-2d58-4d02-b32d-b1662f8d691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    text = standardize_text(text)\n",
    "    # replace usernames/urls\n",
    "    text = replace_usernames(text, filler='username')\n",
    "    text = replace_urls(text, filler='url')\n",
    "    text = asciify_emojis(text)\n",
    "    text = standardize_punctuation(text)\n",
    "    text = text.lower()\n",
    "    text = replace_multi_occurrences(text,'username')\n",
    "    text = replace_multi_occurrences(text, 'url')\n",
    "    text = remove_unicode_symbols(text)\n",
    "    text = remove_accented_characters(text)\n",
    "    return text\n",
    "df['cleaned']  = df['tweet'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61c6d24a-a50b-4e1c-bd6d-863e058cbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd62685e-ea78-4579-b8bc-2f6216c6f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['cleaned']  = test_df['tweet'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ecdebee-73da-47f5-a61a-d94534c74756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'cleaned', 'final_cleaned', 'labels', 'conspiracy', 'country',\n",
       "       'ineffective', 'ingredients', 'mandatory', 'none', 'pharma',\n",
       "       'political', 'religious', 'rushed', 'side-effect', 'unnecessary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e565565-d85c-4694-91ab-ed420c43747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_df_2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f35c267-f131-4bc3-91ec-83ab584850a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\programdata\\anaconda3\\lib\\site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 12.1 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting distlib\n",
      "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
      "     -------------------------------------- 468.9/468.9 kB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: distlib\n",
      "Successfully installed distlib-0.3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-estimator (c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install --upgrade distlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7cdb17-d1a4-415d-98be-e90cfe8ec0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
